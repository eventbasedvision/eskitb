<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers">
  <meta name="keywords" content="Event Cameras, Object Tracking, Neuromorphic Vision, Ski Tracking">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="media/css/bulma.min.css">
  <link rel="stylesheet" href="media/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="media/css/bulma-slider.min.css">
  <link rel="stylesheet" href="media/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="media/css/index.css">
  <link rel="icon" href="media/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="media/js/fontawesome.all.min.js"></script>
  <script src="media/js/bulma-carousel.min.js"></script>
  <script src="media/js/bulma-slider.min.js"></script>
  <script src="media/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers
          </h1>

          <div class="is-size-7 publication-authors">
						<p>(Accepted in IEEE/CVF Winter Conference on Applications of Computer Vision (WACV-2026))</p>
					</div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Krishnaa-Vinod">Krishna Vinod</a>,</span>
            <span class="author-block">
              <a href="https://github.com/joe-rabbit">Joseph Raj Vishal</a>,</span>
            <span class="author-block">
              <a href="https://github.com/Kaustav97">Kaustav Chanda</a>,
            </span>
            <span class="author-block">
              <a href="https://github.com/Prithvijai">Prithvi Jai Ramesh</a>,
            </span>
            <span class="author-block">
              <a href="https://github.com/UMD-Cognitive-Robot">Yezhou Yang</a>,
            </span>
            <span class="author-block">
              <a href="https://github.com/chakravarthi589">Bharatesh Chakravarthi</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Arizona State University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2601.06647"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/eventbasedvision/eskitb"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://www.dropbox.com/scl/fo/ggk7wcbqb4xrjylvwkbh2/ADvJ0Q8hUV6mS_yChJz_u2I?rlkey=1fbsjthks57e3mucxrsz5&st=i5h5avm3&dl=0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="media/images/Teaser_image.png" class="center" />
      <h2 class="subtitles has-text-centered">
        <strong>eSkiTB</strong>
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Event cameras are bio-inspired sensors that asynchronously capture per-pixel brightness changes at microsecond precision, offering low latency, high temporal resolution, and resilience to motion blurâ€”critical for tracking fast-moving objects. Despite their advantages, the lack of standardized benchmarks and datasets for event-based tracking has limited their integration into computer vision pipelines. This paper introduces eSkiTB, a specialized benchmark for event-based tracking in winter sports, focusing on ski jumpers, freestyle skiers, and alpine skiers. Derived from broadcast footage covering varied lighting, weather, and clutter conditions, eSkiTB includes 300 sequences with 240 for training, 30 for validation, and 30 for testing. Using the v2e event simulator under the iso-informational constraint, we ensure neuromorphic fidelity while preserving temporal resolution. We evaluate state-of-the-art event and frame-based trackers, demonstrating the efficacy of spiking neural networks and the value of fine-tuning on domain-specific data. eSkiTB advances neuromorphic vision research and establishes a foundation for event-based tracking in extreme-motion scenarios.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">Dataset Pipeline</h2>
					<img src="media/images/v2e pipeline without slowmo.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p> 
              We convert broadcast RGB videos to events using the v2e simulator under the iso-informational constraint, ensuring no neural interpolation artifacts. The pipeline includes upsampling to 1000 fps via frame repetition, adaptive color-to-grayscale conversion using the CIE luminance formula, and cubic spline interpolation for smooth bounding box trajectories at 1ms resolution. Events are stored in HDF5 format with microsecond-level timestamps (t, x, y, p), providing high temporal precision while maintaining compatibility with standard neuromorphic processing tools. This approach preserves the temporal fidelity of event cameras without introducing synthetic motion interpolation.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>


  	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">Dataset Statistics</h2>
					<img src="media/images/eSkiTB Statistics.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p>
              eSkiTB contains 300 sequences distributed across three skiing disciplines: Alpine (AL), Freestyle (FS), and Ski Jumping (JP). The dataset is split into 240 training sequences, 30 validation sequences, and 30 test sequences, ensuring comprehensive coverage of diverse scenarios including varying speeds, camera angles, weather conditions, and occlusion patterns. Each sequence contains synchronized event data at 1280x720 resolution with bounding box annotations interpolated at 1ms intervals. The dataset captures challenging tracking scenarios with athletes moving at speeds up to 140 km/h, rapid camera movements, complex backgrounds with spectators and gates, and extreme lighting variations from bright snow to shadowed slopes.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

  	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">Why Event Cameras for Ski Tracking?</h2>
					<img src="media/images/rgb_artifacts.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p>
              Ski tracking presents unique challenges that conventional RGB cameras struggle with: extreme speeds (up to 140 km/h), rapid illumination changes from bright snow to dark shadows, severe motion blur during fast movements, and compression artifacts in broadcast footage. Event cameras address these limitations through their bio-inspired design: (1) <strong>High Temporal Resolution</strong> - Microsecond-level event capture eliminates motion blur by registering brightness changes independently per pixel; (2) <strong>Low Latency</strong> - Asynchronous operation enables real-time tracking with minimal delay; (3) <strong>High Dynamic Range</strong> - Superior performance across extreme lighting conditions (120+ dB vs 60 dB for standard cameras); (4) <strong>Efficient Encoding</strong> - Sparse event representation focuses computational resources on motion-relevant regions. The figure above demonstrates how RGB frames suffer from motion blur and compression artifacts, while event representations maintain clean, high-fidelity motion information.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

  	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">Benchmark Results</h2>
					<img src="media/images/tracking_samples.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p>
              We evaluate state-of-the-art event-based and frame-based trackers on eSkiTB. <strong>SDTrack</strong> (spiking transformer) achieves <strong>0.711 IoU</strong> after fine-tuning on eSkiTB (+0.399 improvement over pretrained baseline), demonstrating the effectiveness of spiking neural networks for event-based tracking. In high-clutter scenarios, SDTrack reaches <strong>0.685 IoU</strong>, outperforming generic STARK RGB by +20.0 points. Frame-based trackers also benefit from domain adaptation: <strong>STARK fine-tuned on eSkiTB events achieves 0.795 IoU</strong>, while <strong>STARK fine-tuned on ski-specific RGB data reaches 0.829 IoU</strong>, establishing the upper bound. These results highlight two key findings: (1) event-based representations provide robust tracking under extreme motion and lighting conditions, and (2) domain-specific fine-tuning is critical for both event and frame-based methods to handle ski tracking challenges.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

  	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">Qualitative Results</h2>
					<img src="media/images/Qualitative_samples.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p>
              Fine-tuned trackers demonstrate robust performance across challenging scenarios. The figure above shows tracking results on test sequences featuring: (1) <strong>Occlusions</strong> - Skiers passing behind gates, trees, or other obstacles; (2) <strong>High Clutter</strong> - Dense backgrounds with spectators, advertising banners, and complex textures; (3) <strong>Scale Variation</strong> - Dramatic changes in target size due to camera zoom and perspective; (4) <strong>Harsh Weather</strong> - Snowfall, fog, and low-light conditions. The visualizations demonstrate how event-based tracking maintains accurate bounding box predictions even when RGB-based methods fail due to motion blur or lighting challenges. Green boxes indicate successful tracking with high IoU overlap, while challenging frames showcase the robustness of fine-tuned models.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

  	<section class="hero teaser">
		<div class="container is-max-desktop">

			<div class="columns is-centered has-text-centered">
				<div class="column is-four-fifths">
					<hr>
					<h2 class="title is-3">Temporal Analysis</h2>
					<img src="media/images/IoU-over-time curves.png" class="center" />
					<div class="content has-text-justified">
						<br>
						<p>
              We analyze tracking performance over time to understand model robustness across sequence duration. The IoU-over-time curves reveal that fine-tuned models maintain stable tracking performance throughout extended sequences, while baseline models show degradation over time. Event-based trackers exhibit particularly strong temporal consistency due to their ability to handle motion blur and lighting changes. The analysis demonstrates that domain-specific training not only improves average IoU but also reduces performance variance across different sequence segments, indicating better generalization to the full range of ski tracking challenges present in eSkiTB.
						</p>
					</div>
				</div>
			</div>
			<hr>
		</div>
	</section>

  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      </div>
    </div>
    <hr>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{vinod2026eskitb,
  title     = {eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers},
  author    = {Vinod, Krishna and Vishal, Joseph Raj and Chanda, Kaustav and Ramesh, Prithvi Jai and Yang, Yezhou and Chakravarthi, Bharatesh},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  year      = {2026}
}
</code></pre>
  </div>
</section>


	<footer class="footer">
		<div class="container">
			<div class="content has-text-centered">
			</div>
			<div class="columns is-centered">
				<div class="column is-8">
					<div class="content">
						<center>
							<p>
								This website is licensed under a <a rel="license"
									href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
									Commons Attribution-ShareAlike 4.0 International License</a>.
								This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
								We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing
								this template.
							</p>
						</center>
					</div>
				</div>
				</p>
			</div>
		</div>
		</div>
		</div>
	</footer>

</body>
</html>
